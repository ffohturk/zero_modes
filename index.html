<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Weight pruning / zero modes paper</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #34495e;
        }
        .abstract {
            background-color: #f2f2f2;
            padding: 15px;
            border-radius: 5px;
            margin-bottom: 20px;
        }
        .links {
            margin-top: 30px;
        }
        .links a {
            display: inline-block;
            margin-right: 20px;
            padding: 10px 15px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            transition: background-color 0.3s ease;
        }
        .links a:hover {
            background-color: #2980b9;
        }
    </style>
</head>
<body>
    <header>
        <h1>Do not prune but interpret weights that did not change</h1>
        <h2>Jorrit Kruthoff</h2>
    </header>

    <main>
        <section class="abstract">
            <h2>Abstract</h2>
            <p>
                We empirically show that language models have a tiny set of weights that did not change after finetuning. Such weights, which we call zero modes, are essential for the model's functionality. They can be linked to various mechanisms that are ought to be learned during pretraining, such as induction heads or a neuron firing on the first sentence of an input. We make these claims concrete in one and two layer toy models trained on general code and finetuned on Python and three popular open-source pretrained-finetuned pairs of models. We find many similarities between the toy and large language models. For the LLMs, we not only show how the zero modes impact their performance on a wide variety of tasks, but also how they can affect hidden states and help select interesting (and universal) neurons.
            </p>
        </section>

        <section class="links">
            <h2>Paper Resources</h2>
            <a href="Do_not_prune_but_interpret.pdf" target="_blank">Download Full Draft (PDF)</a>
            <a href="visualisation.html">Neuron visualizations</a>
        </section>
    </main>
</body>
</html>
